Model: Default

> ACT AS a profesional writter and transcribe, fix the identation the following text: Okay, hello Cloud Gurus and welcome to this lecture. In this lecture, we're going to review everything that we've just learned about databases in this section of the course. So let's start with RDS. So RDS database types come in SQL Server, Oracle, MySQL, PostgreSQL, MariaDB and Amazon Aurora. RDS is for online transaction processing workloads. So this is where you are basically processing lots of small transactions like customer orders, banking transactions, payments and booking systems. It's not really suitable for online analytics processing. So for that, you would use something like Redshift for data warehousing and OLAP tasks, like analyzing large amounts of data, reporting and sales forecasting. And we're gonna cover off Redshift later on in the course. Now you'll always be quizzed on where you should use read replicas versus where you should use multi AZ. So read replicas, this is where you are scaling your read performance, so it's primarily used for scaling, not for disaster recovery. It requires automatic backups to be turned on. So if you can't for some reason create a read replica of your primary database, it just means you haven't turned on automatic backups. And then multiple read replicas are supported. So you can have up to five read replicas for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server. In terms of multi AZ versus read replica, like I said earlier, So an exact copy of your production database is in another availability zone for multi AZ. It's used only for disaster recovery. And in the event of a failure, RDS will automatically fail over to the standby instance. Whereas with read replicas, a read only copy of your primary database in the same availability zone, cross availability zones or cross regions is available. And of course, you use this to increase or scale your read performance. So if you get a scenario question about your database bottlenecking and how you can get around it, think automatically of read replicas. So it's great for read heavy workloads and it takes the load off your primary database for read only workloads. So this could be things like business intelligence reporting jobs, et cetera. So moving on to Aurora, just remember going into your exam that Aurora is Amazon's proprietary database. It's something that they have created themselves. that's compatible with MySQL as well as PostgreSQL. It always has two copies of your data in each availability zone with a minimum of three availability zones. So you always have six copies of your data. So it's very, very redundant. You can also share Aurora snapshots with other AWS accounts and you have three types of replicas available with Aurora. So you've got Aurora replicas themselves but then you can also create MySQL replicas and PostgresQL replicas. An automated failover is actually only available when you have Aurora replicas. And Aurora has automated backups turned on by default. And you can also take snapshots with Aurora and you can share these snapshots with other AWS accounts. Now don't forget about Aurora serverless and what the use cases are for it. So it provides a relatively simple cost-effective option for infrequent, intermittent, or unpredictable workloads. If you see any scenario based question where it's talking about setting up a serverless database, I want you to automatically think of Aurora and we will cover off what serverless is later on in the course as well. So moving on to DynamoDB, four facts about DynamoDB. It's stored on SSD storage. It's spread across three geographically distinct data centers and you can have eventually consistent reads, which is what you get by default. But then you can also have strongly consistent reads. And just in case you need a reminder of the difference between eventually consistent reads and strongly consistent reads, eventually consistent means that consistency across all copies of data is usually reached within about a second and repeating a read after a short time should return the updated data. And this gives you the best read performance. However, if your application needs to automatically get any changes to the data, you want to turn on strongly consistent reads. This will return a result that reflects all writes that have received a successful response prior to the read. Moving on to DynamoDB transactions, this is where you have multiple all or nothing operations, so it's good for things like financial transactions or fulfilling orders. You have three options for your reads, you have eventual consistency, strong consistency, and then transactional. You have two options for your writes, we have standard and transactional. You can have 25 items using DynamoDB transactions or 4 megs of data. If you see any scenario question that mentions ACID requirements, I want you to think of DynamoDB transactions. DynamoDB transactions provides developers with atomicity, consistency, isolation and durability across one or more tables within an AWS account or region. If you see something about acid requirements with DynamoDB, you need to use DynamoDB transactions. This basically means you've got all or nothing transactions. So you either need to add an item to all the tables in one transaction, but if one of those tables fails for whatever reason, you don't want one item going into one table, not into another table because that wouldn't have acid consistency. So you need to make sure you've got DynamoDB transactions turned on. And that way either the write will happen or it won't. Moving on to DynamoDB on-demand backups and restores. This is where you can back up your DynamoDB database with full backups at any time. It has zero impact on your table performance or availability. And you have consistent within seconds and your backups are retained until deleted. And it operates within the same region as the source tables. When you're creating a DynamoDB backup, it will be in the same region as the source table. Then we have DynamoDB point in time recovery. This is really cool technology. It protects against accidental writes or deletes. You can restore your DynamoDB database to any point in the last 35 days. The backups are incremental. It's not enabled by default, however, you will have to go and turn it on. The last restorable rate is going to be five minutes in the past. You then learn about DynamoDB Streams, and this is where you can maintain first in, first out records of your data. So as you go and add transactions, basically it's given a sequence number, these sequences are stored in a stream and the data is broken up into shards. So it's just time ordered sequences of item level changes in a table. Every shard is stored for 24 hours and your stream records are going to consist of things like inserts, updates and deletes. You can combine this with a Lambda function to add functionality like stored procedures. So if you're a SQL DBA or a MySQL DBA and you've used stored procedures before, essentially you can have the same tech kind of concept with DynamoDB. You can do that using a Lambda function and we will explore what Lambda is later on in the course. So with global tables, this is where you have managed multi-master multi-region replication. It's designed where you've got applications that are globally distributed. You need to have DynamoDB streams turned on, so it's based off DynamoDB streams. And this basically allows you to have multi-region redundancy for disaster recovery or higher availability. And you don't have to rewrite your application as we saw. This is all built-in functionality within DynamoDB. And your application latency typically will be under one second. And the key thing here is to remember, if they're talking about if you want to add redundancy to DynamoDB, you'll need to turn on global tables. If global tables isn't turning on, you need to make sure that you've enabled DynamoDB streams. So moving on to running Mongo compatible databases in the cloud, do you remember the name of the database technology you should use? Well, it is Amazon DocumentDB. So a typical question might be around moving your MongoDB database to AWS. So you've got it on premise, you might I want to use something like AWS Database Migration Service, which we're going to cover off later on in the migration section of the course. And then where are you going to deploy it? Well, you use Amazon DocumentDB for MongoDB workloads. So if you see a scenario question where they're talking about migrating MongoDB from on-premise to AWS, I want you to think of Amazon DocumentDB. If you see a scenario question about Cassandra, So this would be about migrating a big data Cassandra cluster to AWS. I want you to think of AWS key spaces. Moving on to Neptune. This is often used as a distractor. So if the scenario is not talking about graph databases, do not select Neptune as an answer. Neptune definitely comes up a lot in the exam, but it is almost nine times out of 10 a distractor. So just remember, if it's not talking about graph databases, don't select Neptune. you only need to know what Neptune is at a very high level. Onto QLDB, this is also often used as a distractor. So this is quantum ledger database technology. So if the scenario is not talking about an immutable database, do not select QLDB. Again, you only need to know what this is at a very high level. If you see a scenario question about time series data, then I want you to think of time stream. So this might be where you've got temperature sensors from all around the world at weather stations and it's collecting the temperature every second across thousands of locations. You need to store it in a database. Then I want you to think of time stream. So that is it for this section of the course. You've done very, very well. If you have any questions, please let me know. If not, feel free to move on to the next section. Thank you.

Okay, hello Cloud Gurus and welcome to this lecture. In this lecture, we're going to review everything that we've just learned about databases in this section of the course. So let's start with RDS.

So RDS database types come in SQL Server, Oracle, MySQL, PostgreSQL, MariaDB and Amazon Aurora. RDS is for online transaction processing workloads. So this is where you are basically processing lots of small transactions like customer orders, banking transactions, payments and booking systems. It's not really suitable for online analytics processing. So for that, you would use something like Redshift for data warehousing and OLAP tasks, like analyzing large amounts of data, reporting and sales forecasting. And we're gonna cover off Redshift later on in the course.

Now you'll always be quizzed on where you should use read replicas versus where you should use multi AZ. So read replicas, this is where you are scaling your read performance, so it's primarily used for scaling, not for disaster recovery. It requires automatic backups to be turned on. So if you can't for some reason create a read replica of your primary database, it just means you haven't turned on automatic backups. And then multiple read replicas are supported. So you can have up to five read replicas for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server.

In terms of multi AZ versus read replica, like I said earlier, So an exact copy of your production database is in another availability zone for multi AZ. It's used only for disaster recovery. And in the event of a failure, RDS will automatically fail over to the standby instance. Whereas with read replicas, a read only copy of your primary database in the same availability zone, cross availability zones or cross regions is available. And of course, you use this to increase or scale your read performance. So if you get a scenario question about your database bottlenecking and how you can get around it, think automatically of read replicas. So it's great for read heavy workloads and it takes the load off your primary database for read only workloads. This could be things like business intelligence reporting jobs, etc.

So moving on to Aurora, just remember going into your exam that Aurora is Amazon's proprietary database. It's something that they have created themselves. that's compatible with MySQL as well as PostgreSQL. It always has two copies of your data in each availability zone with a minimum of three availability zones. So you always have six copies of your data. So it's very, very redundant. You can also share Aurora snapshots with other AWS accounts and you have three types of replicas available with Aurora. So you've got Aurora replicas themselves but then you can also create MySQL replicas and PostgresQL replicas. An automated failover is actually only available when you have Aurora replicas. And Aurora has automated backups turned on by default. And you can also take snapshots with Aurora and you can share these snapshots with other AWS accounts. Now don't forget about Aurora serverless and what the use cases are for it. So it provides a relatively simple cost-effective option for infrequent, intermittent, or unpredictable workloads. If you see any scenario based question where it's talking about setting up a serverless database, I want you to automatically think of Aurora and we will cover off what serverless is later on in the course as well.

So moving on to DynamoDB, four facts about DynamoDB. It's stored on SSD storage. It's spread across three geographically distinct data centers and you can have eventually consistent reads, which is what you get by default. But then you can also have strongly consistent reads. And just in case you need a reminder of the difference between eventually consistent reads and strongly consistent reads, eventually consistent means that consistency across all copies of data is

> continue

usually reached within about a second and repeating a read after a short time should return the updated data. And this gives you the best read performance. However, if your application needs to automatically get any changes to the data, you want to turn on strongly consistent reads. This will return a result that reflects all writes that have received a successful response prior to the read.

Moving on to DynamoDB transactions, this is where you have multiple all or nothing operations, so it's good for things like financial transactions or fulfilling orders. You have three options for your reads, you have eventual consistency, strong consistency, and then transactional. You have two options for your writes, we have standard and transactional. You can have 25 items using DynamoDB transactions or 4 megs of data. If you see any scenario question that mentions ACID requirements, I want you to think of DynamoDB transactions. DynamoDB transactions provide developers with atomicity, consistency, isolation, and durability across one or more tables within an AWS account or region. If you see something about acid requirements with DynamoDB, you need to use DynamoDB transactions. This basically means you've got all or nothing transactions. So you either need to add an item to all the tables in one transaction, but if one of those tables fails for whatever reason, you don't want one item going into one table, not into another table because that wouldn't have acid consistency. So you need to make sure you've got DynamoDB transactions turned on. And that way either the write will happen or it won't.

Moving on to DynamoDB on-demand backups and restores. This is where you can back up your DynamoDB database with full backups at any time. It has zero impact on your table performance or availability. And you have consistent within seconds and your backups are retained until deleted. And it operates within the same region as the source tables. When you're creating a DynamoDB backup, it will be in the same region as the source table. Then we have DynamoDB point in time recovery. This is really cool technology. It protects against accidental writes or deletes. You can restore your DynamoDB database to any point in the last 35 days. The backups are incremental. It's not enabled by default, however, you will have to go and turn it on. The last restorable rate is going to be five minutes in the past.

You then learn about DynamoDB Streams, and this is where you can maintain first in, first out records of your data. So as you go and add transactions, basically it's given a sequence number, these sequences are stored in a stream and the data is broken up into shards. So it's just time ordered sequences of item level changes in a table. Every shard is stored for 24 hours and your stream records are going to consist of things like inserts, updates, and deletes. You can combine this with a Lambda function to add functionality like stored procedures. So if you're a SQL DBA or a MySQL DBA and you've used stored procedures before, essentially you can have the same tech kind of concept with DynamoDB. You can do that using a Lambda function and we will explore what Lambda is later on in the course.

So with global tables, this is where you have managed multi-master multi-region replication. It's designed where you've got applications that are globally distributed. You need to have DynamoDB streams turned on, so it's based off DynamoDB streams. And this basically allows you to have multi-region redundancy for disaster recovery or higher availability. And you don't have to rewrite your application as we saw. This is all built-in functionality within DynamoDB. And your application latency typically will be under one second. And the key thing here is to remember, if they're talking about if you

> continue

want to add redundancy to DynamoDB, you'll need to turn on global tables. If global tables isn't turned on, you need to make sure that you've enabled DynamoDB streams.

Moving on to running Mongo compatible databases in the cloud, do you remember the name of the database technology you should use? Well, it is Amazon DocumentDB. A typical question might be around moving your MongoDB database to AWS. So if you have it on premise, you might want to use something like AWS Database Migration Service, which we're going to cover off later on in the migration section of the course. And then where are you going to deploy it? Well, you use Amazon DocumentDB for MongoDB workloads. So if you see a scenario question where they're talking about migrating MongoDB from on-premise to AWS, I want you to think of Amazon DocumentDB. If you see a scenario question about Cassandra, this would be about migrating a big data Cassandra cluster to AWS. I want you to think of AWS Keyspaces.

Moving on to Neptune. This is often used as a distractor. So if the scenario is not talking about graph databases, do not select Neptune as an answer. Neptune definitely comes up a lot in the exam, but it is almost nine times out of 10 a distractor. So just remember, if it's not talking about graph databases, don't select Neptune. You only need to know what Neptune is at a very high level.

Onto QLDB, this is also often used as a distractor. So this is quantum ledger database technology. So if the scenario is not talking about an immutable database, do not select QLDB. Again, you only need to know what this is at a very high level.

If you see a scenario question about time series data, then I want you to think of TimeStream. So this might be where you've got temperature sensors from all around the world at weather stations and it's collecting the temperature every second across thousands of locations. You need to store it in a database. Then I want you to think of TimeStream.

That is it for this section of the course. You've done very, very well. If you have any questions, please let me know. If not, feel free to move on to the next section. Thank you.

> 2 / 2
> 
> now i want to use that text and I want you to act as a AWS Certified solution architect and that information about the certification's material for AWS Solution Architect Associated certification, check,verify and correct, the information if it’s necessary to have relevant and important information. If there is any information missing, add all necessary material to improve the study information, please list all the key points to lear/remember

Sure, I can help you with that. Here are the key points related to AWS Solution Architect Associated certification based on the transcribed text:

1.  RDS database types: AWS Solution Architect Associated certification covers the different RDS database types, including SQL Server, Oracle, MySQL, PostgreSQL, MariaDB, and Amazon Aurora.
    
2.  RDS for online transaction processing: The certification covers RDS as a suitable option for online transaction processing workloads, where there are many small transactions being processed, such as customer orders, banking transactions, payments, and booking systems.
    
3.  Redshift for online analytics processing: The certification highlights the use of Redshift for online analytics processing tasks such as analyzing large amounts of data, reporting, and sales forecasting.
    
4.  Read replicas and multi-AZ: The certification covers the use of read replicas and multi-AZ for scaling and disaster recovery purposes in RDS. It also emphasizes the differences between them and the scenarios where one should be used over the other.
    
5.  Amazon Aurora: The certification covers Amazon Aurora, a proprietary database that is compatible with MySQL and PostgreSQL. It emphasizes its benefits, such as having two copies of data in each availability zone with a minimum of three availability zones, and its different types of replicas.
    
6.  DynamoDB: The certification covers DynamoDB, a NoSQL database that is stored on SSD storage and is spread across three geographically distinct data centers. It also covers DynamoDB transactions, backups, and restores, as well as point-in-time recovery and streams.
    
7.  Amazon DocumentDB: The certification covers Amazon DocumentDB as the recommended database technology for running MongoDB workloads in the cloud.
    
8.  TimeStream: The certification covers TimeStream, a database service designed for storing and querying time series data.
    
9.  Distractors: The certification cautions against selecting Neptune and QLDB as answers unless the scenario specifically involves graph databases or immutable databases, respectively.
    

Remembering these key points will help you prepare for the AWS Solution Architect Associated certification exam.

> explain where you should use read replicas versus where you should use multi AZ in tabular way

Sure, here's a table to summarize where you should use read replicas versus where you should use multi AZ in RDS:


|  | Read Replicas | Multi-AZ |
| --- | --- | --- |
| Purpose | Scaling read performance | Disaster recovery |
| Backup | Requires automatic backups to be turned on | Automatic backups are turned on by default |
| Number of Copies | Multiple read replicas are supported (up to 5) | Exact copy of production database in another availability zone |
| Failover | Not automatic, requires manual promotion | Automatic failover to standby instance |
| Suitable for | Read-heavy workloads, business intelligence reporting | Write-heavy workloads, critical production systems |


I hope this helps!

> Explain eventually consistent reads, and strongly consistent reads in DynamoDB in tabular way markdown syntax’s

Sure, here's a table to explain eventually consistent reads and strongly consistent reads in DynamoDB:


|  | Eventually Consistent Reads | Strongly Consistent Reads |
| --- | --- | --- |
| Consistency | Data is usually consistent across all copies within a second, but not guaranteed to be so. | Ensures that all copies of data are consistent before returning the result. |
| Performance | Provides the best read performance because it does not wait for all copies to be updated. | Slower than eventually consistent reads, as it waits for all copies to be updated before returning the result. |
| Use Cases | Good for scenarios where data can be slightly out-of-date, such as recommendation engines. | Good for scenarios where data needs to be up-to-date, such as real-time data processing. |
| Syntax | Use the `ConsistentRead` parameter set to `false` to perform eventually consistent reads. | Use the `ConsistentRead` parameter set to `true` to perform strongly consistent reads. |
| Cost | Cheaper than strongly consistent reads because it requires less processing power. | More expensive than eventually consistent reads because it requires more processing power. |
| Availability | Available for all operations in DynamoDB, and is the default setting for all reads. | Available only for table reads, and requires additional capacity for read operations. |
| Repeating a Read | Repeating a read may return the updated data if the consistency has been reached. | Repeating a read immediately after may still return the previous data because the consistency hasn't been reached yet. |
| Automatic Scaling | Supports automatic scaling with Global Tables and DynamoDB Streams. | Supports automatic scaling with Global Tables and DynamoDB Streams. |
| Availability Zones | Available within a region. | Available within a region. |
| Limitations | Not suitable for scenarios where data consistency is critical, such as financial transactions. | Not suitable for scenarios where data can be slightly out-of-date, such as recommendation engines. |


I hope this table helps! Let me know if you have any further questions.

> explain DynamoDB Streams in tabular way

Sure, here's a table to explain DynamoDB Streams:


|  | DynamoDB Streams |
| --- | --- |
| Purpose | Maintains a first in, first out record of data changes in a table. |
| Sequence Numbers | Given to every transaction as a time-ordered identifier. |
| Shards | Breaks up the data into time-ordered sequences of item-level changes. |
| Retention | Each shard is stored for 24 hours. |
| Data in Records | Inserts, updates, and deletes. |
| Lambda Functions | Can be used to add functionality, similar to stored procedures in relational databases. |
| Scalability | Supports automatic scaling with Global Tables and DynamoDB Streams. |
| Use Cases | Good for scenarios where you need to maintain a history of all changes to your data. |
| Syntax | Enabled by creating a stream on the table. |
| Cost | Charged based on the number of read requests made to the stream. |
| Availability | Available within a region. |
| Limitations | Cannot store data permanently, only retains data for 24 hours. |
| Difference from SNS | SNS is used for notifications when events occur, whereas DynamoDB Streams maintains a record of all data changes. |


I hope this helps! Let me know if you have any further questions.